{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e96b91-47cd-47f1-84c0-86c80cf3d6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AZURE_API_KEY: d5c1ae66fe5a41fd82f61e0b9fe88470\n",
      "AZURE_API_VERSION: 2024-02-15-preview\n",
      "AZURE_DEPLOYMENT: gpt-4t\n",
      "AZURE_ENDPOINT: https://itp-azopenai-ne.openai.azure.com/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vlad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Notebook 2: Embedding and Chunking\n",
    "\n",
    "# Run the first notebook to load environment variables and imports\n",
    "%run \"environment_setup.ipynb\"\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Define an embedding function with batching\n",
    "class SentenceTransformerEmbeddings:\n",
    "    def __init__(self, model, batch_size=2):\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def embed_documents(self, documents):\n",
    "        embeddings = []\n",
    "        for start_idx in range(0, len(documents), self.batch_size):\n",
    "            batch = documents[start_idx:start_idx + self.batch_size]\n",
    "            batch_embeddings = self.model.encode(batch, show_progress_bar=True).tolist()\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, query):\n",
    "        return self.model.encode(query, show_progress_bar=True).tolist()\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.model.encode(text, show_progress_bar=True).tolist()\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model, batch_size=1)  # Batch size set to 2\n",
    "\n",
    "# Define the ChunkingHelper class\n",
    "class ChunkingHelper:\n",
    "    def __init__(self, file_path: str, chunking_method: str, embedding_function):\n",
    "        self.file_path = file_path\n",
    "        self.chunking_method = chunking_method\n",
    "        self.embedding_function = embedding_function\n",
    "        self.loader = PyMuPDFLoader(file_path=file_path)\n",
    "        self.docs = self.loader.load()\n",
    "        self.chunks = []\n",
    "\n",
    "    def chunk_document(self):\n",
    "        if self.chunking_method == 'standard_deviation':\n",
    "            text_splitter = SemanticChunker(self.embedding_function, breakpoint_threshold_type='standard_deviation')\n",
    "        elif self.chunking_method == 'recursive_character':\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown chunking method: {self.chunking_method}\")\n",
    "        \n",
    "        self.chunks = text_splitter.split_documents(self.docs)\n",
    "\n",
    "    def get_chunks(self):\n",
    "        if not self.chunks:\n",
    "            self.chunk_document()\n",
    "        return self.chunks\n",
    "\n",
    "# Paths to the directories containing the PDF files\n",
    "directories = {\n",
    "    'chroma': r\"E:\\RepoFisiereMulte\\FisiereTotal\",\n",
    "    'faiss': r\"E:\\RepoFisiereMulte\\FisiereTotal\",\n",
    "    'pinecone': r\"E:\\RepoFisiereMulte\\FisiereTotal\"\n",
    "}\n",
    "\n",
    "all_texts = {key: [] for key in directories}\n",
    "all_metadatas = {key: [] for key in directories}\n",
    "\n",
    "def process_pdfs(directory_path, all_texts, all_metadatas, chunking_method):\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            chunking_helper = ChunkingHelper(file_path, chunking_method, embedding_function)\n",
    "            splits = chunking_helper.get_chunks()\n",
    "            for i, split in enumerate(splits):\n",
    "                split.metadata['chunk_id'] = i\n",
    "                split.metadata['file'] = filename\n",
    "                all_texts.append(split)\n",
    "                all_metadatas.append(split.metadata)\n",
    "\n",
    "# Process PDFs with the specified chunking method\n",
    "chunking_method = 'recursive_character'  # or 'recursive_character'\n",
    "for key, directory in directories.items():\n",
    "    process_pdfs(directory, all_texts[key], all_metadatas[key], chunking_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e75b294-e0b4-4711-8f49-97c725064438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
